# 1. 指定 Spark 去 Maven 仓库下载 S3 相关的依赖包
#    这个配置会告诉 Spark 自动下载 hadoop-aws 和 aws-java-sdk-bundle
spark.jars.packages                  org.apache.hadoop:hadoop-aws:3.4.1

# 2. 配置 S3a 文件系统实现
spark.hadoop.fs.s3a.impl             org.apache.hadoop.fs.s3a.S3AFileSystem

# 3. 配置 AWS 凭证 (强烈推荐使用 IAM Role for Service Accounts - IRSA)
#    如果你的 K8s 集群配置了 IRSA，这是最安全的方式
spark.hadoop.fs.s3a.aws.credentials.provider com.amazonaws.auth.WebIdentityTokenCredentialsProvider

# 4. 如果不使用 IRSA，可以配置 Access Key 和 Secret Key
#    注意：直接写在这里不安全，更好的方式是通过环境变量或 Kubernetes Secrets 传入
spark.hadoop.fs.s3a.access.key       cXFVWCBKY6xlUVjuc8Qk
spark.hadoop.fs.s3a.secret.key       Hx1pYxR6sCHo4NAXqRZ1jlT8Ue6SQk6BqWxz7GKY

# 5. （可选）配置你的 S3 Endpoint，如果使用兼容 S3 的存储（如 MinIO）
spark.hadoop.fs.s3a.endpoint         http://minio.default.svc.cluster.local:9000

spark.driver.extraClassPath   /spark-jars/*
spark.executor.extraClassPath /spark-jars/*


