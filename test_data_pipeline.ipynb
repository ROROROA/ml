{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 開發與測試 Notebook\n",
    "\n",
    "本 Notebook 用於交互式地開發、測試和驗證 `data_pipeline` 中的核心 ETL 邏輯。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "--- ✅ 正在调用 x.py 中【新定义的】get_spark_session_connect ---\n",
      "      尝试连接到: sc://localhost:15002\n",
      "      ✅ 成功连接到 Spark Connect Server, 版本: 4.0.0\n",
      "開發環境準備就緒！\n"
     ]
    }
   ],
   "source": [
    "# Cell 1: 魔法命令和導入\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "\n",
    "import os\n",
    "from datetime import datetime, timedelta\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# 導入我們需要測試的核心任務\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_spark_session_connect() -> SparkSession:\n",
    "    \"\"\"\n",
    "    这是我们【新】的 get_spark_session 实现，使用 Spark Connect。\n",
    "    \"\"\"\n",
    "    print(\"--- ✅ 正在调用 x.py 中【新定义的】get_spark_session_connect ---\")\n",
    "    SPARK_CONNECT_URL = os.getenv(\"SPARK_CONNECT_URL\", \"sc://localhost:31002\")\n",
    "    print(f\"      尝试连接到: {SPARK_CONNECT_URL}\")\n",
    "    try:\n",
    "        spark = SparkSession.builder.remote(SPARK_CONNECT_URL).getOrCreate()\n",
    "        print(f\"      ✅ 成功连接到 Spark Connect Server, 版本: {spark.version}\")\n",
    "        return spark\n",
    "    except Exception as e:\n",
    "        print(f\"      ❌ 连接 Spark Connect Server 失败: {e}\")\n",
    "        raise\n",
    "# import src.pipelines.data_pipeline as data_pipeline\n",
    "# data_pipeline.get_spark_session = get_spark_session_connect\n",
    "\n",
    "spark = get_spark_session_connect()\n",
    "\n",
    "print(\"開發環境準備就緒！\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\30630\\ml\\feature_repo\\entities.py:7: DeprecationWarning: Entity value_type will be mandatory in the next release. Please specify a value_type for entity 'user_id'.\n",
      "  user_entity = Entity(name=\"user_id\", description=\"The user entity for MovieLens\")\n",
      "c:\\Users\\30630\\ml\\feature_repo\\entities.py:8: DeprecationWarning: Entity value_type will be mandatory in the next release. Please specify a value_type for entity 'movie_id'.\n",
      "  movie_entity = Entity(name=\"movie_id\", description=\"The movie entity for MovieLens\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Start testing process_features_for_single_day...\n",
      "\n",
      "--- [DEBUG] Schema before applying window function ---\n",
      "root\n",
      " |-- user_id: integer (nullable = true)\n",
      " |-- timestamp: date (nullable = false)\n",
      " |-- timestamp_long: long (nullable = false)\n",
      " |-- rating: float (nullable = true)\n",
      "\n",
      "\n",
      "--- [DEBUG] Sample data before applying window function (first 5 rows) ---\n",
      "+-------+----------+--------------+------+\n",
      "|user_id|timestamp |timestamp_long|rating|\n",
      "+-------+----------+--------------+------+\n",
      "|33019  |2019-01-15|1547510400    |5.0   |\n",
      "|33019  |2019-01-15|1547510400    |4.5   |\n",
      "|33019  |2019-01-15|1547510400    |5.0   |\n",
      "|33019  |2019-01-15|1547510400    |5.0   |\n",
      "|33019  |2019-01-15|1547510400    |4.0   |\n",
      "+-------+----------+--------------+------+\n",
      "only showing top 5 rows\n",
      "\n",
      " 函數執行成功！\n"
     ]
    }
   ],
   "source": [
    "# Cell 3: 運行您的核心任務進行測試\n",
    "from src.pipelines.data_pipeline import process_features_for_single_day\n",
    "if spark:\n",
    "    try:\n",
    "        print(\"\\nStart testing process_features_for_single_day...\")\n",
    "\n",
    "        test_date = \"2019-01-15\"\n",
    "        \n",
    "\n",
    "        process_features_for_single_day.fn(\n",
    "            target_date_str=test_date,\n",
    "            feature_groups_to_process=[\"user_rolling_ratings\"],\n",
    "            spark=spark\n",
    "\n",
    "        )\n",
    "        \n",
    "        print(\"\\n 函數執行成功！\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"\\n 函數執行失敗: {e}\")\n",
    "        # 在這裡您可以進行斷點調試\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔍 驗證輸出數據...\n",
      "+-------+-------------------+---------------------+----------+\n",
      "|user_id|avg_rating_past_30d|rating_count_past_30d| timestamp|\n",
      "+-------+-------------------+---------------------+----------+\n",
      "|  33019|                4.7|                    5|2019-01-15|\n",
      "|  33183|               NULL|                    0|2019-01-15|\n",
      "|  33294|  3.647727272727273|                   44|2019-01-15|\n",
      "+-------+-------------------+---------------------+----------+\n",
      "\n",
      "\n",
      " shutting down spark session...\n"
     ]
    }
   ],
   "source": [
    "# Cell 4: 驗證結果\n",
    "# 函數執行成功後，您可以在這裡用 Spark SQL 來檢查輸出是否符合預期\n",
    "\n",
    "if spark:\n",
    "    try:\n",
    "        print(\"\\n🔍 驗證輸出數據...\")\n",
    "        output_table = \"feature_store.user_rolling_ratings_historical\"\n",
    "        result_df = spark.sql(f\"\"\"\n",
    "            SELECT * FROM {output_table} \n",
    "            WHERE timestamp = to_date('{test_date}', 'yyyy-MM-dd') \n",
    "            ORDER BY user_id\n",
    "            LIMIT 10\n",
    "        \"\"\")\n",
    "\n",
    "        result_df.show()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ 驗證失敗: {e}\")\n",
    "    finally:\n",
    "        # 在 Notebook 的末尾關閉會話\n",
    "        print(\"\\n shutting down spark session...\")\n",
    "        spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
