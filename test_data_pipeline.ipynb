{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### é–‹ç™¼èˆ‡æ¸¬è©¦ Notebook\n",
    "\n",
    "æœ¬ Notebook ç”¨æ–¼äº¤äº’å¼åœ°é–‹ç™¼ã€æ¸¬è©¦å’Œé©—è­‰ `data_pipeline` ä¸­çš„æ ¸å¿ƒ ETL é‚è¼¯ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "--- âœ… æ­£åœ¨è°ƒç”¨ x.py ä¸­ã€æ–°å®šä¹‰çš„ã€‘get_spark_session_connect ---\n",
      "      å°è¯•è¿æ¥åˆ°: sc://localhost:15002\n",
      "      âœ… æˆåŠŸè¿æ¥åˆ° Spark Connect Server, ç‰ˆæœ¬: 4.0.0\n",
      "é–‹ç™¼ç’°å¢ƒæº–å‚™å°±ç·’ï¼\n"
     ]
    }
   ],
   "source": [
    "# Cell 1: é­”æ³•å‘½ä»¤å’Œå°å…¥\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "\n",
    "import os\n",
    "from datetime import datetime, timedelta\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# å°å…¥æˆ‘å€‘éœ€è¦æ¸¬è©¦çš„æ ¸å¿ƒä»»å‹™\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_spark_session_connect() -> SparkSession:\n",
    "    \"\"\"\n",
    "    è¿™æ˜¯æˆ‘ä»¬ã€æ–°ã€‘çš„ get_spark_session å®ç°ï¼Œä½¿ç”¨ Spark Connectã€‚\n",
    "    \"\"\"\n",
    "    print(\"--- âœ… æ­£åœ¨è°ƒç”¨ x.py ä¸­ã€æ–°å®šä¹‰çš„ã€‘get_spark_session_connect ---\")\n",
    "    SPARK_CONNECT_URL = os.getenv(\"SPARK_CONNECT_URL\", \"sc://localhost:31002\")\n",
    "    print(f\"      å°è¯•è¿æ¥åˆ°: {SPARK_CONNECT_URL}\")\n",
    "    try:\n",
    "        spark = SparkSession.builder.remote(SPARK_CONNECT_URL).getOrCreate()\n",
    "        print(f\"      âœ… æˆåŠŸè¿æ¥åˆ° Spark Connect Server, ç‰ˆæœ¬: {spark.version}\")\n",
    "        return spark\n",
    "    except Exception as e:\n",
    "        print(f\"      âŒ è¿æ¥ Spark Connect Server å¤±è´¥: {e}\")\n",
    "        raise\n",
    "# import src.pipelines.data_pipeline as data_pipeline\n",
    "# data_pipeline.get_spark_session = get_spark_session_connect\n",
    "\n",
    "spark = get_spark_session_connect()\n",
    "\n",
    "print(\"é–‹ç™¼ç’°å¢ƒæº–å‚™å°±ç·’ï¼\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\30630\\ml\\feature_repo\\entities.py:7: DeprecationWarning: Entity value_type will be mandatory in the next release. Please specify a value_type for entity 'user_id'.\n",
      "  user_entity = Entity(name=\"user_id\", description=\"The user entity for MovieLens\")\n",
      "c:\\Users\\30630\\ml\\feature_repo\\entities.py:8: DeprecationWarning: Entity value_type will be mandatory in the next release. Please specify a value_type for entity 'movie_id'.\n",
      "  movie_entity = Entity(name=\"movie_id\", description=\"The movie entity for MovieLens\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Start testing process_features_for_single_day...\n",
      "\n",
      "--- [DEBUG] Schema before applying window function ---\n",
      "root\n",
      " |-- user_id: integer (nullable = true)\n",
      " |-- timestamp: date (nullable = false)\n",
      " |-- timestamp_long: long (nullable = false)\n",
      " |-- rating: float (nullable = true)\n",
      "\n",
      "\n",
      "--- [DEBUG] Sample data before applying window function (first 5 rows) ---\n",
      "+-------+----------+--------------+------+\n",
      "|user_id|timestamp |timestamp_long|rating|\n",
      "+-------+----------+--------------+------+\n",
      "|33019  |2019-01-15|1547510400    |5.0   |\n",
      "|33019  |2019-01-15|1547510400    |4.5   |\n",
      "|33019  |2019-01-15|1547510400    |5.0   |\n",
      "|33019  |2019-01-15|1547510400    |5.0   |\n",
      "|33019  |2019-01-15|1547510400    |4.0   |\n",
      "+-------+----------+--------------+------+\n",
      "only showing top 5 rows\n",
      "\n",
      " å‡½æ•¸åŸ·è¡ŒæˆåŠŸï¼\n"
     ]
    }
   ],
   "source": [
    "# Cell 3: é‹è¡Œæ‚¨çš„æ ¸å¿ƒä»»å‹™é€²è¡Œæ¸¬è©¦\n",
    "from src.pipelines.data_pipeline import process_features_for_single_day\n",
    "if spark:\n",
    "    try:\n",
    "        print(\"\\nStart testing process_features_for_single_day...\")\n",
    "\n",
    "        test_date = \"2019-01-15\"\n",
    "        \n",
    "\n",
    "        process_features_for_single_day.fn(\n",
    "            target_date_str=test_date,\n",
    "            feature_groups_to_process=[\"user_rolling_ratings\"],\n",
    "            spark=spark\n",
    "\n",
    "        )\n",
    "        \n",
    "        print(\"\\n å‡½æ•¸åŸ·è¡ŒæˆåŠŸï¼\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"\\n å‡½æ•¸åŸ·è¡Œå¤±æ•—: {e}\")\n",
    "        # åœ¨é€™è£¡æ‚¨å¯ä»¥é€²è¡Œæ–·é»èª¿è©¦\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ” é©—è­‰è¼¸å‡ºæ•¸æ“š...\n",
      "+-------+-------------------+---------------------+----------+\n",
      "|user_id|avg_rating_past_30d|rating_count_past_30d| timestamp|\n",
      "+-------+-------------------+---------------------+----------+\n",
      "|  33019|                4.7|                    5|2019-01-15|\n",
      "|  33183|               NULL|                    0|2019-01-15|\n",
      "|  33294|  3.647727272727273|                   44|2019-01-15|\n",
      "+-------+-------------------+---------------------+----------+\n",
      "\n",
      "\n",
      " shutting down spark session...\n"
     ]
    }
   ],
   "source": [
    "# Cell 4: é©—è­‰çµæœ\n",
    "# å‡½æ•¸åŸ·è¡ŒæˆåŠŸå¾Œï¼Œæ‚¨å¯ä»¥åœ¨é€™è£¡ç”¨ Spark SQL ä¾†æª¢æŸ¥è¼¸å‡ºæ˜¯å¦ç¬¦åˆé æœŸ\n",
    "\n",
    "if spark:\n",
    "    try:\n",
    "        print(\"\\nğŸ” é©—è­‰è¼¸å‡ºæ•¸æ“š...\")\n",
    "        output_table = \"feature_store.user_rolling_ratings_historical\"\n",
    "        result_df = spark.sql(f\"\"\"\n",
    "            SELECT * FROM {output_table} \n",
    "            WHERE timestamp = to_date('{test_date}', 'yyyy-MM-dd') \n",
    "            ORDER BY user_id\n",
    "            LIMIT 10\n",
    "        \"\"\")\n",
    "\n",
    "        result_df.show()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ é©—è­‰å¤±æ•—: {e}\")\n",
    "    finally:\n",
    "        # åœ¨ Notebook çš„æœ«å°¾é—œé–‰æœƒè©±\n",
    "        print(\"\\n shutting down spark session...\")\n",
    "        spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
