import findspark
findspark.init()

from pyspark.sql import SparkSession
from pyspark.sql.functions import from_unixtime, to_date, col, current_timestamp

# work with interactive-shell-integration.yaml no need port forward

def setup_spark_session(warehouse_bucket):
    """
    é…ç½®å¹¶è¿”å›ä¸€ä¸ªè¿æ¥åˆ° K8s é›†ç¾¤ã€S3 å’Œ PostgreSQL Hive Metastore çš„ SparkSessionã€‚
    è¿™ä¸ªå‡½æ•°æ¥æ”¶ä¸€ä¸ªå‚æ•°æ¥å®šä¹‰æ•°æ®ä»“åº“çš„ä½ç½®ã€‚
    """
    # K8s å’Œ Jar æŒ‚è½½é…ç½® (ä¸ä¹‹å‰è°ƒè¯•æ—¶ä¿æŒä¸€è‡´)
    host_jar_path_in_k8s = "/run/desktop/mnt/host/c/Users/30630/.ivy2.5.2/jars"
    mount_path_in_container = "/opt/spark/jars-pv"
    
    # PostgreSQL Metastore è¿æ¥ä¿¡æ¯
    PG_HOST = "mlflow-postgres-postgresql.default.svc.cluster.local"
    PG_PORT = "5432"
    PG_DATABASE = "spark"
    PG_USER = "mlflow_user"
    PG_PASSWORD = "mlflow_password"
    
    print("å¼€å§‹æ„å»ºç”Ÿäº§ç¯å¢ƒ SparkSession...")
    
    spark = (
        SparkSession.builder
        .appName("S3-To-Hive-MovieLens-ETL")
        .enableHiveSupport()

        # --- K8s, Classpath, Mounts, S3 é…ç½® ---
        .master("k8s://https://kubernetes.default.svc.cluster.local:443")
        .config("spark.kubernetes.container.image", "my-spark-jupyter:latest")
        .config("spark.kubernetes.authenticate.driver.serviceAccountName", "spark-operator-spark")
        .config("spark.driver.extraClassPath", f"{mount_path_in_container}/*")
        .config("spark.executor.extraClassPath", f"{mount_path_in_container}/*")
        .config("spark.kubernetes.executor.volumes.hostPath.shared-jars-hostpath.options.path", host_jar_path_in_k8s)
        .config("spark.kubernetes.executor.volumes.hostPath.shared-jars-hostpath.mount.path", mount_path_in_container)
        .config("spark.hadoop.fs.s3a.endpoint", "http://minio.default.svc.cluster.local:9000")
        .config("spark.hadoop.fs.s3a.access.key", "cXFVWCBKY6xlUVjuc8Qk")
        .config("spark.hadoop.fs.s3a.secret.key", "Hx1pYxR6sCHo4NAXqRZ1jlT8Ue6SQk6BqWxz7GKY")
        .config("spark.hadoop.fs.s3a.path.style.access", "true")

        # --- Hive Metastore é…ç½® ---
        .config("spark.sql.catalogImplementation", "hive")
        .config("spark.hadoop.javax.jdo.option.ConnectionDriverName", "org.postgresql.Driver")
        .config("spark.hadoop.javax.jdo.option.ConnectionURL", f"jdbc:postgresql://{PG_HOST}:{PG_PORT}/{PG_DATABASE}")
        .config("spark.hadoop.javax.jdo.option.ConnectionUserName", PG_USER)
        .config("spark.hadoop.javax.jdo.option.ConnectionPassword", PG_PASSWORD)
        .config("spark.hadoop.datanucleus.autoCreateSchema", "true")
        .config("spark.hadoop.datanucleus.autoCreateTables", "true")
        .config("spark.hadoop.datanucleus.fixedDatastore", "false")
        .config("spark.hadoop.hive.metastore.schema.verification", "false")
        
        # --- æ•°æ®ä»“åº“ä½ç½® ---
        # Hiveè¡¨çš„æ•°æ®å°†é»˜è®¤å­˜å‚¨åœ¨è¿™ä¸ª S3 Bucket çš„æ ¹ç›®å½•ä¸‹
        .config("spark.sql.warehouse.dir", f"s3a://{warehouse_bucket}/")
        
        .getOrCreate()
    )
    print("âœ… SparkSession æ„å»ºæˆåŠŸï¼")
    return spark

def manage_hive_schema(spark, db_name, tables_to_create):
    """
    ç®¡ç† Hive æ•°æ®åº“å’Œè¡¨ï¼Œç¡®ä¿ä¸€ä¸ªå¹²å‡€çš„è¿è¡Œç¯å¢ƒã€‚
    1. åˆ›å»ºæ•°æ®åº“ï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰ã€‚
    2. åˆ é™¤æ‰€æœ‰ç›®æ ‡è¡¨ï¼ˆå¦‚æœå·²å­˜åœ¨ï¼‰ã€‚
    3. é‡æ–°åˆ›å»ºæ‰€æœ‰ç›®æ ‡è¡¨ã€‚
    """
    print(f"\n--- æ­¥éª¤ 1: å‡†å¤‡æ•°æ®åº“å’Œè¡¨ Schema ---")
    
    # åˆ›å»ºæ•°æ®åº“
    spark.sql(f"CREATE DATABASE IF NOT EXISTS {db_name}")
    print(f"æ•°æ®åº“ '{db_name}' å·²å­˜åœ¨æˆ–åˆ›å»ºæˆåŠŸã€‚")
    
    # åˆ‡æ¢åˆ°è¯¥æ•°æ®åº“
    spark.sql(f"USE {db_name}")
    print(f"å·²åˆ‡æ¢åˆ°æ•°æ®åº“ '{db_name}'ã€‚")

    for table_name, ddl in tables_to_create.items():
        # å…ˆåˆ é™¤å·²å­˜åœ¨çš„è¡¨ï¼Œç¡®ä¿å¹‚ç­‰æ€§
        spark.sql(f"DROP TABLE IF EXISTS {table_name}")
        print(f"æ—§è¡¨ '{table_name}' (å¦‚æœå­˜åœ¨) å·²è¢«åˆ é™¤ã€‚")
        
        # åˆ›å»ºæ–°è¡¨
        spark.sql(ddl)
        print(f"æ–°è¡¨ '{table_name}' å·²æ ¹æ® DDL åˆ›å»ºæˆåŠŸã€‚")

def main():
    """
    ä¸»ETLæµç¨‹: ä»S3è¯»å–æ•°æ®ï¼Œè½¬æ¢åå†™å…¥Hive Metastoreç®¡ç†çš„è¡¨ä¸­ã€‚
    """
    # --- å®šä¹‰å­˜å‚¨ä½ç½® ---
    SOURCE_DATA_BUCKET = "movielens"        # å­˜æ”¾æºCSVæ–‡ä»¶çš„Bucket
    WAREHOUSE_BUCKET = "spark-warehouse"    # å­˜æ”¾å¤„ç†åçš„Parquetè¡¨æ•°æ®çš„Bucket
    
    # è®¾ç½®SparkSessionï¼Œå¹¶æŒ‡å®šæ•°æ®ä»“åº“çš„ä½ç½®
    spark = setup_spark_session(WAREHOUSE_BUCKET)
    
    # --- æ•°æ®åº“å’Œè¡¨å®šä¹‰ ---
    DATABASE_NAME = "raw_data"
    RATINGS_TABLE_NAME = f"{DATABASE_NAME}.movielens_ratings"
    MOVIES_TABLE_NAME = f"{DATABASE_NAME}.movielens_movies"

    # å®šä¹‰æ‰€æœ‰éœ€è¦åˆ›å»ºçš„è¡¨çš„DDLï¼ˆæ•°æ®å®šä¹‰è¯­è¨€ï¼‰
    tables_ddl = {
        RATINGS_TABLE_NAME: f"""
            CREATE TABLE {RATINGS_TABLE_NAME} (
                userId INT,
                movieId INT,
                rating FLOAT,
                timestamp LONG,
                rating_date DATE
            )
            USING PARQUET
            PARTITIONED BY (rating_date)
        """,
        MOVIES_TABLE_NAME: f"""
            CREATE TABLE {MOVIES_TABLE_NAME} (
                movieId INT,
                title STRING,
                genres STRING,
                created_timestamp TIMESTAMP
            )
            USING PARQUET
        """
    }

    # æ‰§è¡Œ Schema ç®¡ç†
    manage_hive_schema(spark, DATABASE_NAME, tables_ddl)

    # --- æ­¥éª¤ 2: å¤„ç† ratings.csv ---
    print(f"\n--- æ­¥éª¤ 2: å¤„ç† ratings æ•°æ® ---")
    # ä»æºæ•°æ®Bucketè¯»å–CSV
    ratings_s3_path = f"s3a://{SOURCE_DATA_BUCKET}/ml-25m/ratings.csv"
    print(f"ä» S3 è¯»å– ratings æ•°æ®: {ratings_s3_path}")

    ratings_df = spark.read.csv(ratings_s3_path, header=True, inferSchema=True).limit(100000)

    ratings_transformed_df = ratings_df \
        .withColumn("rating_timestamp", from_unixtime(col("timestamp"))) \
        .withColumn("rating_date", to_date(col("rating_timestamp"))) \
        .select("userId", "movieId", "rating", "timestamp", "rating_date")
    
    print(f"è½¬æ¢äº† {ratings_transformed_df.count()} æ¡ ratings è®°å½•ã€‚")

    print(f"å¼€å§‹å°† ratings æ•°æ®å†™å…¥åˆ°è¡¨: {RATINGS_TABLE_NAME}")
    # Sparkä¼šè‡ªåŠ¨æ ¹æ®warehouse.diré…ç½®å°†æ•°æ®å†™å…¥åˆ° WAREHOUSE_BUCKET
    ratings_transformed_df.write \
        .mode("overwrite") \
        .insertInto(RATINGS_TABLE_NAME)
    print(f"âœ… ratings æ•°æ®æˆåŠŸå†™å…¥ï¼")


    # --- æ­¥éª¤ 3: å¤„ç† movies.csv ---
    print(f"\n--- æ­¥éª¤ 3: å¤„ç† movies æ•°æ® ---")
    # ä»æºæ•°æ®Bucketè¯»å–CSV
    movies_s3_path = f"s3a://{SOURCE_DATA_BUCKET}/ml-25m/movies.csv"
    print(f"ä» S3 è¯»å– movies æ•°æ®: {movies_s3_path}")
    
    movies_df = spark.read.csv(movies_s3_path, header=True, inferSchema=True)

    movies_transformed_df = movies_df \
        .withColumn("created_timestamp", current_timestamp()) \
        .select("movieId", "title", "genres", "created_timestamp")

    print(f"è½¬æ¢äº† {movies_transformed_df.count()} æ¡ movies è®°å½•ã€‚")
    
    print(f"å¼€å§‹å°† movies æ•°æ®å†™å…¥åˆ°è¡¨: {MOVIES_TABLE_NAME}")
    # Sparkä¼šè‡ªåŠ¨æ ¹æ®warehouse.diré…ç½®å°†æ•°æ®å†™å…¥åˆ° WAREHOUSE_BUCKET
    movies_transformed_df.write \
        .mode("overwrite") \
        .insertInto(MOVIES_TABLE_NAME)
    print(f"âœ… movies æ•°æ®æˆåŠŸå†™å…¥ï¼")


    print("\nğŸ‰ å…¨éƒ¨æ•°æ®å¤„ç†å’ŒåŠ è½½ä»»åŠ¡æˆåŠŸå®Œæˆï¼")
    spark.stop()

if __name__ == "__main__":
    main()

